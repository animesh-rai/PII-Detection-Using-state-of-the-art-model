{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGyTFCaK5VdmxMyThpBXH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/animesh-rai/x23194545_Sensitive_data_detection/blob/main/sensitive_data_inference_using_all_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting google drive to load finetuned models for senstive data detection"
      ],
      "metadata": {
        "id": "0Cjiy5uFcO_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjlrJyvlPruE",
        "outputId": "6c57d356-2343-4ff1-d4d4-4b8e405d06c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with DistilBERT"
      ],
      "metadata": {
        "id": "hQBsm6-RbhqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "model_path = \"/content/drive/My Drive/PII_models/distilbert_sensitive_data_detection_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "\n",
        "# Input text\n",
        "text = \"Sensitive data such as credit card numbers should be protected.\"\n",
        "\n",
        "# Tokenize input\n",
        "tokens = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,\n",
        "    is_split_into_words=False\n",
        ")\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokens)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Map predictions to labels\n",
        "id2label = model.config.id2label  # Mapping from label IDs to label names\n",
        "predicted_labels = [id2label[label] for label in predictions[0].tolist()]\n",
        "tokens_decoded = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
        "\n",
        "# Post-process to clean and merge subwords\n",
        "special_tokens = tokenizer.all_special_tokens\n",
        "cleaned_results = []\n",
        "current_word = \"\"\n",
        "current_label = None\n",
        "\n",
        "for token, label in zip(tokens_decoded, predicted_labels):\n",
        "    if token in special_tokens:\n",
        "        continue  # Skip special tokens\n",
        "    if token.startswith(\"##\"):  # Subword continuation\n",
        "        current_word += token[2:]  # Append without ##\n",
        "    else:  # New word\n",
        "        if current_word:  # Add the previous word and label\n",
        "            cleaned_results.append((current_word, current_label))\n",
        "        current_word = token  # Start a new word\n",
        "        current_label = label\n",
        "\n",
        "# Append the last word\n",
        "if current_word:\n",
        "    cleaned_results.append((current_word, current_label))\n",
        "\n",
        "# Display results\n",
        "print(\"\\nCleaned Predictions:\")\n",
        "for word, label in cleaned_results:\n",
        "    print(f\"{word}: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcKBqUGNZ5kV",
        "outputId": "fe3ab159-8aa2-48c1-9d63-c8d805a69132"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaned Predictions:\n",
            "sensitive: O\n",
            "data: O\n",
            "such: O\n",
            "as: O\n",
            "credit: O\n",
            "card: O\n",
            "numbers: O\n",
            "should: O\n",
            "be: O\n",
            "protected: O\n",
            ".: O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with DeBERTa"
      ],
      "metadata": {
        "id": "7JXreFryb4Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "save_directory = \"/content/drive/My Drive/PII_models/deberta_b_sensitive_data_detection_model\"\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    save_directory,\n",
        "    ignore_mismatched_sizes=True  # Ignore size mismatches if any\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "\n",
        "# Input text for inference\n",
        "text = \"The@ is the mastermind in a criminal.com case loudge on student portal. \"\n",
        "\n",
        "# Tokenize the input\n",
        "tokens = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,\n",
        "    is_split_into_words=False\n",
        ")\n",
        "\n",
        "# Move the model and input tokens to the same device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "tokens = {key: val.to(device) for key, val in tokens.items()}\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokens)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Map predictions to labels\n",
        "id2label = model.config.id2label  # Ensure this is correctly set in your config\n",
        "predicted_labels = [id2label[label] for label in predictions[0].tolist()]\n",
        "tokens_decoded = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
        "\n",
        "\n",
        "# Display result in nice format# Filter out special tokens and merge subwords\n",
        "\n",
        "special_tokens = tokenizer.all_special_tokens\n",
        "word_labels = []\n",
        "current_word = \"\"\n",
        "current_label = None\n",
        "\n",
        "for token, label in zip(tokens_decoded, predicted_labels):\n",
        "    if token in special_tokens:\n",
        "        continue  # Skip special tokens\n",
        "\n",
        "    if token.startswith(\"▁\"):  # New word\n",
        "        if current_word:  # Append the last word and label\n",
        "            word_labels.append((current_word, current_label))\n",
        "        current_word = token.lstrip(\"▁\")  # Remove prefix\n",
        "        current_label = label\n",
        "    else:  # Continuation of the previous word\n",
        "        current_word += token\n",
        "\n",
        "# Append the last word\n",
        "if current_word:\n",
        "    word_labels.append((current_word, current_label))\n",
        "\n",
        "# Display results\n",
        "print(\"Cleaned Output:\")\n",
        "for word, label in word_labels:\n",
        "    print(f\"{word}: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_46DXO0iQQJ5",
        "outputId": "a7f5188c-0abb-4ab2-c92c-e85c50628057"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at /content/drive/My Drive/PII_models/deberta_b_sensitive_data_detection_model and are newly initialized: ['deberta.encoder.LayerNorm.bias', 'deberta.encoder.LayerNorm.weight', 'deberta.encoder.layer.0.attention.self.key_proj.bias', 'deberta.encoder.layer.0.attention.self.key_proj.weight', 'deberta.encoder.layer.0.attention.self.query_proj.bias', 'deberta.encoder.layer.0.attention.self.query_proj.weight', 'deberta.encoder.layer.0.attention.self.value_proj.bias', 'deberta.encoder.layer.0.attention.self.value_proj.weight', 'deberta.encoder.layer.1.attention.self.key_proj.bias', 'deberta.encoder.layer.1.attention.self.key_proj.weight', 'deberta.encoder.layer.1.attention.self.query_proj.bias', 'deberta.encoder.layer.1.attention.self.query_proj.weight', 'deberta.encoder.layer.1.attention.self.value_proj.bias', 'deberta.encoder.layer.1.attention.self.value_proj.weight', 'deberta.encoder.layer.10.attention.self.key_proj.bias', 'deberta.encoder.layer.10.attention.self.key_proj.weight', 'deberta.encoder.layer.10.attention.self.query_proj.bias', 'deberta.encoder.layer.10.attention.self.query_proj.weight', 'deberta.encoder.layer.10.attention.self.value_proj.bias', 'deberta.encoder.layer.10.attention.self.value_proj.weight', 'deberta.encoder.layer.11.attention.self.key_proj.bias', 'deberta.encoder.layer.11.attention.self.key_proj.weight', 'deberta.encoder.layer.11.attention.self.query_proj.bias', 'deberta.encoder.layer.11.attention.self.query_proj.weight', 'deberta.encoder.layer.11.attention.self.value_proj.bias', 'deberta.encoder.layer.11.attention.self.value_proj.weight', 'deberta.encoder.layer.2.attention.self.key_proj.bias', 'deberta.encoder.layer.2.attention.self.key_proj.weight', 'deberta.encoder.layer.2.attention.self.query_proj.bias', 'deberta.encoder.layer.2.attention.self.query_proj.weight', 'deberta.encoder.layer.2.attention.self.value_proj.bias', 'deberta.encoder.layer.2.attention.self.value_proj.weight', 'deberta.encoder.layer.3.attention.self.key_proj.bias', 'deberta.encoder.layer.3.attention.self.key_proj.weight', 'deberta.encoder.layer.3.attention.self.query_proj.bias', 'deberta.encoder.layer.3.attention.self.query_proj.weight', 'deberta.encoder.layer.3.attention.self.value_proj.bias', 'deberta.encoder.layer.3.attention.self.value_proj.weight', 'deberta.encoder.layer.4.attention.self.key_proj.bias', 'deberta.encoder.layer.4.attention.self.key_proj.weight', 'deberta.encoder.layer.4.attention.self.query_proj.bias', 'deberta.encoder.layer.4.attention.self.query_proj.weight', 'deberta.encoder.layer.4.attention.self.value_proj.bias', 'deberta.encoder.layer.4.attention.self.value_proj.weight', 'deberta.encoder.layer.5.attention.self.key_proj.bias', 'deberta.encoder.layer.5.attention.self.key_proj.weight', 'deberta.encoder.layer.5.attention.self.query_proj.bias', 'deberta.encoder.layer.5.attention.self.query_proj.weight', 'deberta.encoder.layer.5.attention.self.value_proj.bias', 'deberta.encoder.layer.5.attention.self.value_proj.weight', 'deberta.encoder.layer.6.attention.self.key_proj.bias', 'deberta.encoder.layer.6.attention.self.key_proj.weight', 'deberta.encoder.layer.6.attention.self.query_proj.bias', 'deberta.encoder.layer.6.attention.self.query_proj.weight', 'deberta.encoder.layer.6.attention.self.value_proj.bias', 'deberta.encoder.layer.6.attention.self.value_proj.weight', 'deberta.encoder.layer.7.attention.self.key_proj.bias', 'deberta.encoder.layer.7.attention.self.key_proj.weight', 'deberta.encoder.layer.7.attention.self.query_proj.bias', 'deberta.encoder.layer.7.attention.self.query_proj.weight', 'deberta.encoder.layer.7.attention.self.value_proj.bias', 'deberta.encoder.layer.7.attention.self.value_proj.weight', 'deberta.encoder.layer.8.attention.self.key_proj.bias', 'deberta.encoder.layer.8.attention.self.key_proj.weight', 'deberta.encoder.layer.8.attention.self.query_proj.bias', 'deberta.encoder.layer.8.attention.self.query_proj.weight', 'deberta.encoder.layer.8.attention.self.value_proj.bias', 'deberta.encoder.layer.8.attention.self.value_proj.weight', 'deberta.encoder.layer.9.attention.self.key_proj.bias', 'deberta.encoder.layer.9.attention.self.key_proj.weight', 'deberta.encoder.layer.9.attention.self.query_proj.bias', 'deberta.encoder.layer.9.attention.self.query_proj.weight', 'deberta.encoder.layer.9.attention.self.value_proj.bias', 'deberta.encoder.layer.9.attention.self.value_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at /content/drive/My Drive/PII_models/deberta_b_sensitive_data_detection_model and are newly initialized because the shapes did not match:\n",
            "- deberta.encoder.rel_embeddings.weight: found shape torch.Size([1024, 768]) in the checkpoint and torch.Size([512, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Output:\n",
            "The@: B-EMAIL\n",
            "is: B-ID_NUM\n",
            "the: B-EMAIL\n",
            "mastermind: B-ID_NUM\n",
            "in: B-ID_NUM\n",
            "a: I-URL_PERSONAL\n",
            "criminal.com: B-PHONE_NUM\n",
            "case: I-PHONE_NUM\n",
            "loudge: I-URL_PERSONAL\n",
            "on: B-NAME_STUDENT\n",
            "student: I-URL_PERSONAL\n",
            "portal.: B-PHONE_NUM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with RoBERTa"
      ],
      "metadata": {
        "id": "PB1I5cG1b_yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "save_directory = \"/content/drive/My Drive/PII_models/roberta_sensitive_data_detection_model\"\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    save_directory,\n",
        "    ignore_mismatched_sizes=True  # Ignore size mismatches if any\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "\n",
        "# Input text for inference\n",
        "text = \"john.doe@gmail.com should be protected 86868547475454\"\n",
        "\n",
        "# Tokenize the input\n",
        "tokens = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,\n",
        "    is_split_into_words=False\n",
        ")\n",
        "\n",
        "# Move the model and input tokens to the same device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "tokens = {key: val.to(device) for key, val in tokens.items()}\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokens)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Map predictions to labels\n",
        "id2label = model.config.id2label  # Ensure this is correctly set in your config\n",
        "predicted_labels = [id2label[label] for label in predictions[0].tolist()]\n",
        "tokens_decoded = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
        "\n",
        "# Post-process to merge subwords and remove special tokens\n",
        "special_tokens = tokenizer.all_special_tokens\n",
        "cleaned_results = []\n",
        "current_word = \"\"\n",
        "current_label = None\n",
        "\n",
        "for token, label in zip(tokens_decoded, predicted_labels):\n",
        "    if token in special_tokens:\n",
        "        continue  # Skip special tokens like <s>, </s>\n",
        "    if token.startswith(\"Ġ\"):  # Start of a new word\n",
        "        if current_word:  # Append the previous word and label\n",
        "            cleaned_results.append((current_word, current_label))\n",
        "        current_word = token.lstrip(\"Ġ\")  # Remove Ġ prefix\n",
        "        current_label = label\n",
        "    else:  # Continuation of the previous word\n",
        "        current_word += token\n",
        "\n",
        "# Append the last word\n",
        "if current_word:\n",
        "    cleaned_results.append((current_word, current_label))\n",
        "\n",
        "# Display the cleaned results\n",
        "print(\"\\nCleaned Predictions:\")\n",
        "for word, label in cleaned_results:\n",
        "    print(f\"{word}: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyd7JdCYaBQb",
        "outputId": "a6d33ecc-ea02-46a2-acf4-ff84f6e8b794"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaned Predictions:\n",
            "john.doe@gmail.com: B-NAME_STUDENT\n",
            "should: O\n",
            "be: O\n",
            "protected: O\n",
            "86868547475454: O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with Longformer"
      ],
      "metadata": {
        "id": "OoHYwzGTcF7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "# Specify the saved directory\n",
        "save_directory = \"/content/drive/My Drive/PII_models/longformer_sensitive_data_detection_model\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = AutoModelForTokenClassification.from_pretrained(save_directory)\n",
        "\n",
        "# Move model to the appropriate device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully!\")\n",
        "\n",
        "# Input text for inference\n",
        "text = \"Sensitive data like credit card numbers, addresses, or personal emails such as john.doe@gmail.com should be protected.\"\n",
        "\n",
        "# Tokenize the input\n",
        "tokens = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,  # Adjust based on your model's maximum length\n",
        "    is_split_into_words=False\n",
        ")\n",
        "\n",
        "# Move tokens to the same device as the model\n",
        "tokens = {key: val.to(device) for key, val in tokens.items()}\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokens)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Map predictions to labels\n",
        "id2label = model.config.id2label  # Ensure this mapping exists in the model configuration\n",
        "tokens_decoded = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
        "predicted_labels = [id2label[label] for label in predictions[0].tolist()]\n",
        "\n",
        "# Post-process to clean and merge subwords\n",
        "special_tokens = tokenizer.all_special_tokens\n",
        "results = []\n",
        "current_word = \"\"\n",
        "current_label = None\n",
        "\n",
        "for token, label in zip(tokens_decoded, predicted_labels):\n",
        "    if token in special_tokens:\n",
        "        continue  # Skip special tokens\n",
        "    if token.startswith(\"▁\") or token.startswith(\"Ġ\") or not current_word:\n",
        "        if current_word:  # Append the last word and label\n",
        "            results.append((current_word, current_label))\n",
        "        current_word = token.lstrip(\"▁Ġ\")  # Remove prefix\n",
        "        current_label = label\n",
        "    else:  # Continuation of the previous word\n",
        "        current_word += token\n",
        "\n",
        "# Append the last word\n",
        "if current_word:\n",
        "    results.append((current_word, current_label))\n",
        "\n",
        "# Display results\n",
        "print(\"\\nCleaned Predictions:\")\n",
        "for word, label in results:\n",
        "    print(f\"{word}: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KojP4VuRbQ-8",
        "outputId": "6ecd6cf2-b857-4d02-8c70-002c7f616f8c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully!\n",
            "\n",
            "Cleaned Predictions:\n",
            "Sensitive: O\n",
            "data: O\n",
            "like: O\n",
            "credit: O\n",
            "card: O\n",
            "numbers,: O\n",
            "addresses,: O\n",
            "or: O\n",
            "personal: O\n",
            "emails: O\n",
            "such: O\n",
            "as: O\n",
            "john.doe@gmail.com: B-EMAIL\n",
            "should: O\n",
            "be: O\n",
            "protected.: O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9z4NBYebbRiG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}